/**
 * Responds to any HTTP request.
 *
 * @param {!express:Request} req HTTP request context.
 * @param {!express:Response} res HTTP response context.
 */

const {BigQuery} = require('@google-cloud/bigquery');
const {Storage} = require('@google-cloud/storage');

const date = new Date();
const day = date.getDate();
const month = date.getMonth()+1;

const projectId = 'hitreceiver';   
const dataset = 'hit_dataset';
const tableName = day+'_'+month+'_hit_table';
const bucketName = 'gcf-sources-633603032502-europe-west1';

const options = {
  schema: 'client_id,hit_timestamp:integer,url_path,referrer'
};

//Initialize bigquery object
const bigQuery = new BigQuery({
    projectId: projectId
});

//Initialize cloud storage object
const storage = new Storage({
      projectId: projectId
}).bucket(bucketName);
const fs = require('fs');

exports.GA_Raw_Hits = (req, res) => {
  try {
    // allow access controll for the domain
   res.set('Access-Control-Allow-Origin', '*');
  if (req.method === 'OPTIONS') {
    // Send response to OPTIONS requests
    res.set('Access-Control-Allow-Methods', 'POST');
    res.set('Access-Control-Allow-Headers', 'Content-Type');
    res.set('Access-Control-Max-Age', '3600');
    res.status(204).send('');
  }
  
  //check if body is not null of response
  if(req.body!==null){
    //storing body as hit variable.
    var hit = req.body;

   for (const [key, value] of Object.entries(hit)) {
     console.log(`${key}: ${value}`);
   }

    
    //adding the hit timestampe so that we can get data and time of the hit in our data set
    
    hit['hit_timestamp'] = new Date().getTime();
   //ceci est un test de push 11
    
    console.log("hit string:",hit);

    console.log('hit type',typeof(hit));

    //we will store the hit received in cloud storage.
    impression(hit);


    //create a table if it not exist
    const table1 = bigQuery.dataset(dataset).table(tableName);

    table1.exists().then((data) => {
       const exists = data[0];
       console.log('table existe :',exists);
      if(exists===true){
         pushBqData();
         console.log('table existe ok');
      }
      else{
      console.log('table existe nok');
         //bigQuery.dataset(dataset).createTable(tableName, options, (err, table, apiResponse) => {});
         bigQuery.dataset(dataset).createTable(tableName, options).then((data) => {
           pushBqData();
         });
      
      }
    });
    
    //send the raw hit recieved from GTM to Bigquery
    function pushBqData() {
    console.log("hit string2:",hit)
    bigQuery
        .dataset(dataset)
        .table(tableName)
        .insert(hit)
        .then((data) => console.log("data inserted to big query ", data))
        .catch(err => {
          
          //in case of an error we will store the hit received in cloud storage.
              console.log("Error: can't insert ",err);
              createFile(err, hit);
                    })
     }
  }
}
  catch (e) {
    console.log('inside catch with error: ',e)
    createFile(e, req.body);
  } 
    //res.status(204).send('');
    res.status(200).send('data stored');
  };
  
  //funciton to store hit to cloud bucket.
function createFile(error,entities) {
    var isotime = new Date().toISOString()+"__"+Math.round(Math.random()*10000); 
    var filename = 'ga_raw_hits_errors/'+error+'/'+error+'_'+isotime+'.ndjson';
    
    // also adding filename in the raw hit will help to backtrack.
    entities['filename'] = filename;

    var gcsStream = storage.file(filename).createWriteStream();
   // Object.values( entities ).forEach( (json) => {
      var str = JSON.stringify( entities );
      gcsStream.write( str + '\n' );
   // });
    gcsStream.on('error', (err) => {
      console.error(`${ this.archive }: Error storage file write.`);
      console.error(`${ this.archive }: ${JSON.stringify(err)}`);
    });
    gcsStream.end();
}

function impression(snipet) {
  var filename = 'ga_impression/impression_'+day+'-'+month+'.json';
  var archivo = storage.file(filename).createReadStream({
    encoding: 'ascii',
    start: 0,
    end: 2000
    });
  var gcsStream = storage.file(filename).createWriteStream();

  var str = JSON.stringify( snipet );
  const file1=storage.file(filename);
  var buf = [];
  var histo=undefined;

  file1.exists().then(function(data) {
    histo = data[0];
    console.log('historique:',histo);
     if(histo===true){
     console.log('fichier existe');
       archivo.on('data', function(d) {
         buf += d;
       }).on('end', function() {
       }).pipe(file1.createWriteStream())
          .on('error', function(err) {
          console.error(err);
       })
       .on('finish', function() {
         console.log('finish upload:');
         console.log('data add:',buf+'\n'+str);
         gcsStream.write(buf+'\n'+str);
         gcsStream.on('error', (err) => {
           console.error(`${ this.archive }: Error storage file write.`);
           console.error(`${ this.archive }: ${JSON.stringify(err)}`);
         });

         gcsStream.end();
        }); 
    }
    else{
        console.log('fichier n\'existe pas');
        gcsStream.write(str);
        gcsStream.on('error', (err) => {
           console.error(`${ this.archive }: Error storage file write.`);
           console.error(`${ this.archive }: ${JSON.stringify(err)}`);
         });
         gcsStream.end();
    }
  });
  
}
